QUESTION: How much data did you put in shared memory?
(Blocksize + 2*kernalsize + 1)^2


QUESTION: How much data does each thread copy to shared memory?
(2*kernalsize + 1)^2

QUESTION: How did you handle the necessary overlap between the blocks?
The blocks do not have any overlap, but every block recives a margin of excess data to process for the edges (kernel size).

QUESTION: If we would like to increase the block size, about how big blocks would be safe to use in this case? Why?
32*32 = 1024 (max number of theards possible for the GPU). This is because of direct translation of pixel to a thread

QUESTION: How much speedup did you get over the naive version? For what filter size?
Naive approch - timediff - 2.337248 (16x16) block 32x32 thread, 16x16 thread  timediff - 0.102816

Shared mem approch - timediff - 0.232608 (16x16) block 32x32 thread, 16x16 thread  timediff - 0.110304

QUESTION: Is your access to global memory coalesced? What should you do to get that?
Yes, we are acess it in a linear fashion and so it is coalesced.

QUESTION: How much speedup did you get over the non-separated? For what filter size?

16x16 thread  timediff - 0.110016 (5x5)

QUESTION: Compare the visual result to that of the box filter. Is the image LP-filtered with the weighted kernel noticeably better?

Yes

QUESTION: What was the difference in time to a box filter of the same size (5x5)?

timediff - 0.362496 (16x16 thread  timediff)


QUESTION: If you want to make a weighted kernel customizable by weights from the host, how would you deliver the weights to the GPU?
Pass it as argument by using CudaMalloc.

QUESTION: What kind of algorithm did you implement for finding the median?
finding the median using pixel couting.

QUESTION: What filter size was best for reducing noise?  
4x4






timediff of original
timediff - 0.102944
timediff of our filter
timediff - 0.116640

after the middle speedup
i got 20% speedup
timediff - 0.082656
